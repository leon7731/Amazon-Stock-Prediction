{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data\\FS_Classification_AMZN_Historical_Quarterly_2009_2022_With_Fundamental_Data_Economic_Indicators.csv')\n",
    "\n",
    "# Removing leading and trailing spaces from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Using a regular expression to replace multiple spaces with a single space in all column names\n",
    "df.columns = df.columns.str.replace(r'\\s+', ' ', regex=True)  \n",
    "\n",
    "# # Dropping columns that are not needed\n",
    "df.drop([\"Date\", \"Year\"], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Overview of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_rows = len(df)\n",
    "print(f\"The number of rows is {num_of_rows}\")\n",
    "print('\\n')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: EDA - Missing Values Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3)i): EDA - Show Missing Values in each Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_columns_with_null_values(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Displays the total number of null values for each column in the dataframe,\n",
    "    showing only columns that have null values.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The dataframe to be checked for null values.\n",
    "    \n",
    "    Returns:\n",
    "    - None: Prints the columns with null values and their counts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get total null values in each column\n",
    "    total_null_values = df.isnull().sum()\n",
    "    \n",
    "    # Filter out columns that don't have any null values\n",
    "    columns_with_null = total_null_values[total_null_values > 0].sort_values(ascending=False)\n",
    "    \n",
    "    # Check if there are any columns with null values\n",
    "    if not columns_with_null.empty:\n",
    "        print('-' * 64)\n",
    "        print(\"Total null values in each column (only columns with null values)\")\n",
    "        print('-' * 64)\n",
    "        print(columns_with_null)\n",
    "    else:\n",
    "        print('-' * 64)\n",
    "        print(\"Total null values in each column (only columns with null values)\")\n",
    "        print('-' * 64)\n",
    "        print(\"No columns have null values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get percentage of null values in each column\n",
    "null_values_percentage = df.isnull().mean().round(4).mul(100).sort_values(ascending=False)\n",
    "print('-' * 44)\n",
    "print(\"Percentage(%) of null values in each column\")\n",
    "print('-' * 44)\n",
    "print(null_values_percentage)\n",
    "print('\\n')\n",
    "\n",
    "# Get total null values in each column\n",
    "display_columns_with_null_values(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3)ii): EDA - Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Null Values in the Remaining Columns with the average of the column\n",
    "numeric_df = df.select_dtypes(include=[np.number]) # Select only numeric columns\n",
    "numeric_df.fillna(numeric_df.mean(), inplace=True)  # Fill missing values in numeric columns with the column mean\n",
    "df[numeric_df.columns] = numeric_df # Merge back with non-numeric columns if needed\n",
    "\n",
    "# Get total null values in each column\n",
    "display_columns_with_null_values(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: EDA - Duplicate Values Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4)i): EDA - Show Duplicate Values Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get percentage of duplicate rows\n",
    "total_rows = len(df)\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "duplicate_percentage = (duplicate_rows / total_rows) * 100\n",
    "\n",
    "print('-' * 48)\n",
    "print(\"Percentage(%) of duplicate rows in the DataFrame\")\n",
    "print('-' * 48)\n",
    "print(f\"{duplicate_percentage:.2f}%\")\n",
    "print('\\n')\n",
    "\n",
    "# Get total number of duplicate rows\n",
    "print('-' * 30)\n",
    "print(\"Total number of duplicate rows\")\n",
    "print('-' * 30)\n",
    "print(duplicate_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5): EDA - Feature Scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5)i): EDA - Categorical Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5)ii): EDA - Numerical Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standard_scale_dataframe(dataframe, columns_to_scale):\n",
    "    \"\"\"\n",
    "    Scales the specified columns of the DataFrame using Standard Scaling (Z-score normalization).\n",
    "    :param dataframe: pandas DataFrame\n",
    "    :param columns_to_scale: list of strings, names of columns to scale\n",
    "    :return: DataFrame with scaled columns\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original one\n",
    "    df_scaled = dataframe.copy()\n",
    "\n",
    "    # Initialize the Standard Scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Perform Standard Scaling on specified columns and update the DataFrame\n",
    "    df_scaled[columns_to_scale] = scaler.fit_transform(dataframe[columns_to_scale])\n",
    "\n",
    "    return df_scaled\n",
    "\n",
    "\n",
    "columns = list(df.columns)\n",
    "\n",
    "numerical_features = columns\n",
    "numerical_features.remove('Hybrid_Price_Movement_Class')\n",
    "\n",
    "scaled_df = standard_scale_dataframe(dataframe=df, \n",
    "                                    columns_to_scale=numerical_features)\n",
    "\n",
    "df = scaled_df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9) Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hybrid_Price_Movement_Class'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(\"Hybrid_Price_Movement_Class\", axis=1)\n",
    "y = df[\"Hybrid_Price_Movement_Class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10) XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def Confusion_Matrix_For_Multi_Class_With_Overview(title, y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Create a confusion matrix for multi-class classification with detailed overview.\n",
    "\n",
    "    Parameters:\n",
    "    - title: Title for the confusion matrix plot.\n",
    "    - y_test: True labels of the test data.\n",
    "    - y_pred: Predicted labels of the test data.\n",
    "\n",
    "    Returns:\n",
    "    - A seaborn heatmap representing the confusion matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # Creating the confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Determine class labels\n",
    "    class_labels = np.unique(np.concatenate((y_test, y_pred)))\n",
    "\n",
    "    # Calculate the counts and percentages for the confusion matrix\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n",
    "    \n",
    "    # Calculate TP and FP percentages\n",
    "    TP_percentages = [\"{0:.2%}\".format(value/np.sum(cf_matrix, axis=1)[i]) for i, value in enumerate(np.diag(cf_matrix))]\n",
    "    FP_percentages = [\"{0:.2%}\".format((np.sum(cf_matrix, axis=0)[i] - value)/np.sum(cf_matrix)) for i, value in enumerate(np.diag(cf_matrix))]\n",
    "    \n",
    "    # Combine TP and FP with their percentages\n",
    "    combined_info = []\n",
    "    for i in range(cf_matrix.shape[0]):\n",
    "        for j in range(cf_matrix.shape[1]):\n",
    "            value = cf_matrix[i, j]\n",
    "            if i == j:  # True Positive\n",
    "                combined_info.append(f\"{value}\\n(TP: {TP_percentages[i]})\")\n",
    "            else:  # False Positive\n",
    "                combined_info.append(f\"{value}\\n(FP: {FP_percentages[j]})\")\n",
    "\n",
    "    labels = np.asarray(combined_info).reshape(cf_matrix.shape)\n",
    "\n",
    "    # Plotting the heatmap\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    ax.set_title(f'{title}\\n\\n')\n",
    "    ax.set_xlabel('\\nPredicted Values')\n",
    "    ax.set_ylabel('Actual Values')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "def XGBoost_Train_Evaluate(X_train, X_test, y_train, y_test, \n",
    "                           objective='multi:softmax', num_class=None, \n",
    "                           n_estimators=100, learning_rate=0.1, max_depth=6, \n",
    "                           subsample=1.0, colsample_bytree=1.0, gamma=0, \n",
    "                           reg_alpha=0, reg_lambda=1, verbosity=1, random_state=42, verbose=False):\n",
    "   \n",
    "    model = xgb.XGBClassifier(\n",
    "        objective=objective,\n",
    "        num_class=num_class,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        gamma=gamma,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        verbosity=verbosity,\n",
    "        random_state=random_state,\n",
    "        use_label_encoder=False  # Avoids warning in newer versions of XGBoost\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=verbose)\n",
    "    y_pred = model.predict(X_test).flatten()  # Ensure y_pred is a 1D array\n",
    "    \n",
    "    df = pd.DataFrame({'Actual': y_test.tolist(), 'Predicted': y_pred.tolist()})\n",
    "    \n",
    "    Confusion_Matrix_For_Multi_Class_With_Overview(\"XGBoost Confusion Matrix\", y_test, y_pred)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Best Parameters: {\n",
    "    'n_estimators': 366, \n",
    "    'learning_rate': 0.4594259565422311,\n",
    "    'max_depth': 3, \n",
    "    'subsample': 0.5163571458564111, \n",
    "    'colsample_bytree': 0.5547050899403205, \n",
    "    'gamma': 1.0546559043221597, \n",
    "    'reg_alpha': 0.29203039952794485, \n",
    "    'reg_lambda': 2.1708419038837645\n",
    "    }\n",
    "    \n",
    "Best Score: 0.4429142292773435\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Best Parameters example\n",
    "XGBoost_df = XGBoost_Train_Evaluate(X_train, X_test, y_train, y_test, \n",
    "                                    objective='multi:softmax', \n",
    "                                    num_class=5, # Update based on your specific number of classes\n",
    "                                    n_estimators=366, \n",
    "                                    learning_rate=0.4594, \n",
    "                                    max_depth=3, \n",
    "                                    subsample=0.5163, \n",
    "                                    colsample_bytree=0.5547, \n",
    "                                    gamma=1.0546, \n",
    "                                    reg_alpha=0.2920, \n",
    "                                    reg_lambda=2.1708, \n",
    "                                    verbosity=1, \n",
    "                                    random_state=42, \n",
    "                                    verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11) XGBoost Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# # Suppress Optuna output\n",
    "# optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "\n",
    "# # Objective function for Optuna\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'objective': 'multi:softmax',\n",
    "#         'num_class': 4,  # Update based on your specific number of classes\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "#         'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.5),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "#         'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "#         'gamma': trial.suggest_float('gamma', 0, 10),\n",
    "#         'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "#         'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "#         'random_state': 42,\n",
    "#         'use_label_encoder': False\n",
    "#     }\n",
    "#     model = xgb.XGBClassifier(**params)\n",
    "#     score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_macro', n_jobs=-1, verbose=0)\n",
    "#     return score.mean()\n",
    "\n",
    "\n",
    "\n",
    "# # Create a study and optimize the objective function\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=500, timeout=None, show_progress_bar=True)\n",
    "\n",
    "# # Print the best parameters and the best score\n",
    "# print(\"Best Parameters:\", study.best_params)\n",
    "# print(\"Best Score:\", study.best_value)\n",
    "\n",
    "# # Get the detailed study results\n",
    "# df = study.trials_dataframe()\n",
    "# df_sorted = df.sort_values('value', ascending=False)\n",
    "# df_sorted.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
