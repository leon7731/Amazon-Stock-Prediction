{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data\\FS_Classification_AMZN_Historical_Quarterly_2009_2022_With_Fundamental_Data_Economic_Indicators.csv')\n",
    "\n",
    "# Removing leading and trailing spaces from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Using a regular expression to replace multiple spaces with a single space in all column names\n",
    "df.columns = df.columns.str.replace(r'\\s+', ' ', regex=True)  \n",
    "\n",
    "# # Dropping columns that are not needed\n",
    "df.drop([\"Date\", \"Year\"], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Overview of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_rows = len(df)\n",
    "print(f\"The number of rows is {num_of_rows}\")\n",
    "print('\\n')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: EDA - Missing Values Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3)i): EDA - Show Missing Values in each Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_columns_with_null_values(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Displays the total number of null values for each column in the dataframe,\n",
    "    showing only columns that have null values.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The dataframe to be checked for null values.\n",
    "    \n",
    "    Returns:\n",
    "    - None: Prints the columns with null values and their counts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get total null values in each column\n",
    "    total_null_values = df.isnull().sum()\n",
    "    \n",
    "    # Filter out columns that don't have any null values\n",
    "    columns_with_null = total_null_values[total_null_values > 0].sort_values(ascending=False)\n",
    "    \n",
    "    # Check if there are any columns with null values\n",
    "    if not columns_with_null.empty:\n",
    "        print('-' * 64)\n",
    "        print(\"Total null values in each column (only columns with null values)\")\n",
    "        print('-' * 64)\n",
    "        print(columns_with_null)\n",
    "    else:\n",
    "        print('-' * 64)\n",
    "        print(\"Total null values in each column (only columns with null values)\")\n",
    "        print('-' * 64)\n",
    "        print(\"No columns have null values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get percentage of null values in each column\n",
    "null_values_percentage = df.isnull().mean().round(4).mul(100).sort_values(ascending=False)\n",
    "print('-' * 44)\n",
    "print(\"Percentage(%) of null values in each column\")\n",
    "print('-' * 44)\n",
    "print(null_values_percentage)\n",
    "print('\\n')\n",
    "\n",
    "# Get total null values in each column\n",
    "display_columns_with_null_values(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3)ii): EDA - Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Null Values in the Remaining Columns with the average of the column\n",
    "numeric_df = df.select_dtypes(include=[np.number]) # Select only numeric columns\n",
    "numeric_df.fillna(numeric_df.mean(), inplace=True)  # Fill missing values in numeric columns with the column mean\n",
    "df[numeric_df.columns] = numeric_df # Merge back with non-numeric columns if needed\n",
    "\n",
    "# Get total null values in each column\n",
    "display_columns_with_null_values(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: EDA - Duplicate Values Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4)i): EDA - Show Duplicate Values Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get percentage of duplicate rows\n",
    "total_rows = len(df)\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "duplicate_percentage = (duplicate_rows / total_rows) * 100\n",
    "\n",
    "print('-' * 48)\n",
    "print(\"Percentage(%) of duplicate rows in the DataFrame\")\n",
    "print('-' * 48)\n",
    "print(f\"{duplicate_percentage:.2f}%\")\n",
    "print('\\n')\n",
    "\n",
    "# Get total number of duplicate rows\n",
    "print('-' * 30)\n",
    "print(\"Total number of duplicate rows\")\n",
    "print('-' * 30)\n",
    "print(duplicate_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5): EDA - Feature Scaling \n",
    "\n",
    "* Categorical Feature Engineering/Scaling\n",
    "* Numerical Feature Engineering/Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define preprocessing for numerical and categorical features\n",
    "columns = list(df.columns)\n",
    "\n",
    "numerical_features = columns\n",
    "numerical_features.remove('Hybrid_Price_Movement_Class')\n",
    "\n",
    "# Preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "      \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9) Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hybrid_Price_Movement_Class'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(\"Hybrid_Price_Movement_Class\", axis=1)\n",
    "y = df[\"Hybrid_Price_Movement_Class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10) XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def Confusion_Matrix_For_Multi_Class_With_Overview(title, y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Create a confusion matrix for multi-class classification with detailed overview.\n",
    "\n",
    "    Parameters:\n",
    "    - title: Title for the confusion matrix plot.\n",
    "    - y_test: True labels of the test data.\n",
    "    - y_pred: Predicted labels of the test data.\n",
    "\n",
    "    Returns:\n",
    "    - A seaborn heatmap representing the confusion matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # Creating the confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Determine class labels\n",
    "    class_labels = np.unique(np.concatenate((y_test, y_pred)))\n",
    "\n",
    "    # Calculate the counts and percentages for the confusion matrix\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n",
    "    \n",
    "    # Calculate TP and FP percentages\n",
    "    TP_percentages = [\"{0:.2%}\".format(value/np.sum(cf_matrix, axis=1)[i]) for i, value in enumerate(np.diag(cf_matrix))]\n",
    "    FP_percentages = [\"{0:.2%}\".format((np.sum(cf_matrix, axis=0)[i] - value)/np.sum(cf_matrix)) for i, value in enumerate(np.diag(cf_matrix))]\n",
    "    \n",
    "    # Combine TP and FP with their percentages\n",
    "    combined_info = []\n",
    "    for i in range(cf_matrix.shape[0]):\n",
    "        for j in range(cf_matrix.shape[1]):\n",
    "            value = cf_matrix[i, j]\n",
    "            if i == j:  # True Positive\n",
    "                combined_info.append(f\"{value}\\n(TP: {TP_percentages[i]})\")\n",
    "            else:  # False Positive\n",
    "                combined_info.append(f\"{value}\\n(FP: {FP_percentages[j]})\")\n",
    "\n",
    "    labels = np.asarray(combined_info).reshape(cf_matrix.shape)\n",
    "\n",
    "    # Plotting the heatmap\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    ax.set_title(f'{title}\\n\\n')\n",
    "    ax.set_xlabel('\\nPredicted Values')\n",
    "    ax.set_ylabel('Actual Values')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "# Create a pipeline \n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', xgb.XGBClassifier(\n",
    "        objective='multi:softmax',\n",
    "        num_class=5,  # Update based on your specific number of classes\n",
    "        n_estimators=366,\n",
    "        learning_rate=0.4594,\n",
    "        max_depth=3,\n",
    "        subsample=0.5163,\n",
    "        colsample_bytree=0.5547,\n",
    "        gamma=1.0546,\n",
    "        reg_alpha=0.2920,\n",
    "        reg_lambda=2.1708,\n",
    "        verbosity=1,\n",
    "        random_state=42,\n",
    "        use_label_encoder=False  # Avoids warnings\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Save the model to a file\n",
    "joblib_file = \"Model/xgboost_model_pipeline.joblib\"\n",
    "joblib.dump(pipeline, joblib_file)\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model using the custom confusion matrix function\n",
    "Confusion_Matrix_For_Multi_Class_With_Overview(\"Confusion Matrix\", y_test, y_pred)\n",
    "\n",
    "# Creating Classification Report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
